{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    \"\"\"\n",
    "    Structure and initilization of the NN.\n",
    "    :param layers: A list of neurons in each layer(including the input and output layers)\n",
    "    :returns: The intialized weights and biases of the required choice of the hidden layers\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases = []\n",
    "    num_layers = len(layers) \n",
    "#     with tf.name_scope(scope):\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = xavier_initialisation(size=[layers[l], layers[l+1]])\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)        \n",
    "    return weights, biases\n",
    "    \n",
    "def xavier_initialisation(size):\n",
    "    \"\"\"\n",
    "    Initializes the NN.\n",
    "    :param size: The dimensions\n",
    "    :returns: The intialized weights or biases for one specific hidden layer\n",
    "    \"\"\"\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]        \n",
    "    xavier_stddev = np.sqrt(20./(in_dim + out_dim))\n",
    "    a = tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev))\n",
    "    return a\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    \"\"\"\n",
    "    Forward propogation of the Neural Network\n",
    "    :param : Time and position of the particle and the parameters of the network\n",
    "    :returns: Velocity and pressure\n",
    "    \"\"\"\n",
    "    num_layers = len(weights) \n",
    "    H = x\n",
    "    for l in range(num_layers):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        if l == num_layers - 1:\n",
    "            H = tf.identity(H, name='class')\n",
    "        else:\n",
    "            H = tf.tanh(H)\n",
    "    return H\n",
    "\n",
    "data = pd.read_csv(\"Concrete_Data_missing.csv\", delimiter=',', index_col=0)\n",
    "train_data_x = data.loc[:800, data.columns !='Concrete compressive strength(MPa, megapascals) ']\n",
    "train_data_y = data.loc[:800, ['Concrete compressive strength(MPa, megapascals) ']]\n",
    "\n",
    "test_data_x = data.loc[800:, data.columns !='Concrete compressive strength(MPa, megapascals) '].values\n",
    "test_data_y = data.loc[800:, ['Concrete compressive strength(MPa, megapascals) ']].values\n",
    "\n",
    "iterable = list(np.arange(8))\n",
    "pos_list = list(itertools.combinations(iterable, 6))\n",
    "pos_list = pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class standardscaler():\n",
    "    def fit(self, x_train):\n",
    "        self.mean = np.mean(x_train)\n",
    "        self.scale = np.std(x_train)\n",
    "        \n",
    "    def transform(self, x_test):\n",
    "        return (x_test-self.mean)/self.scale\n",
    "    \n",
    "    def inversetransform(self, x_test):\n",
    "        return x_test*self.scale+self.mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vel_layers = [6, 100, 100, 100, 100, 1]\n",
    "\n",
    "for j in range(len(pos_list)):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.name_scope('scope_%0i' %j):\n",
    "\n",
    "        vel_weights, vel_biases = initialize_NN(vel_layers)\n",
    "\n",
    "        data_x_cur =  train_data_x.iloc[:,map(int, pos_list[j])]\n",
    "        df_train = pd.concat([data_x_cur, train_data_y], axis=1)\n",
    "        train_index = np.random.randint(800, size=800)\n",
    "        oob_index = np.setdiff1d(np.arange(800),train_index)\n",
    "        df_train = df_train.iloc[train_index]\n",
    "        df_val = df_train.iloc[oob_index]\n",
    "        df_train = df_train.dropna().values\n",
    "        df_val = df_val.dropna().values\n",
    "\n",
    "        x_train = df_train[:,:-1]\n",
    "        y_train = df_train[:,-1][:,None]\n",
    "\n",
    "        x_val = df_val[:,:-1]\n",
    "        y_val = df_val[:,-1][:,None]\n",
    "\n",
    "        scaler = standardscaler()\n",
    "        scaler.fit(x_train)\n",
    "        epochs=200\n",
    "        x_pl = tf.placeholder('float',[None,6], name='input_placeholder')\n",
    "        x_test_pl = tf.placeholder('float',[None,8], name='test_placeholder')\n",
    "        y_pl = tf.placeholder('float',[None,1])\n",
    "\n",
    "        cost = tf.losses.mean_squared_error(labels=y_pl, predictions=forward(x_pl, vel_weights, vel_biases))\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "        batch_size = 50\n",
    "\n",
    "        train_accuracy = np.zeros(epochs)\n",
    "        val_accuracy = np.zeros(epochs)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            x_test = tf.stack([x_test_pl[:,a] for a in pos_list[j]], axis=1)\n",
    "\n",
    "            prediction = tf.identity(forward(scaler.transform(x_test), vel_weights, vel_biases), 'predict')\n",
    "            sess.run(tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='scope_%0i' %j)))\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for i in range(int(x_train.shape[0]/batch_size)):\n",
    "                    epoch_x,epoch_y = x_train[i*batch_size:(i+1)*batch_size,:], y_train[i*batch_size:(i+1)*batch_size,:]\n",
    "                    sess.run(optimizer, feed_dict = {x_pl:scaler.transform(epoch_x),y_pl:epoch_y})\n",
    "            error = tf.identity(sess.run(cost, feed_dict = {x_pl:scaler.transform(x_val),y_pl:y_val}), 'val_error')\n",
    "            saver = tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='scope_%0i' %j))\n",
    "            saver.save(sess, './my_test_model%i' %j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_test_model0\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model1\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model2\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model3\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model4\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model5\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model6\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model7\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model8\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model9\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model10\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model11\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model12\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model13\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model14\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model15\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model16\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model17\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model18\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model19\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model20\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model21\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model22\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model23\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model24\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model25\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model26\n",
      "INFO:tensorflow:Restoring parameters from ./my_test_model27\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "result = []\n",
    "for i in range(len(pos_list)):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:    \n",
    "        saver = tf.train.import_meta_graph('my_test_model%0i.meta' %i)\n",
    "        saver.restore(sess, './my_test_model%i' %i)\n",
    "        result.append(sess.run('scope_%i/predict:0' %i, feed_dict={'scope_%i/test_placeholder:0' %i: test_data_x}))\n",
    "        error.append(sess.run('scope_%i/val_error:0'%i))\n",
    "accuracy = np.max(error)-error\n",
    "normalized = normalize(accuracy[:,np.newaxis], axis=0, norm='l1').ravel()\n",
    "final_prediction = np.sum(np.concatenate([result[i] for i in range(len(result))], axis=1)*normalized, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.827807874850556\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(final_prediction, test_data_y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
